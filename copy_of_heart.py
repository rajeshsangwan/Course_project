# -*- coding: utf-8 -*-
"""Copy of Heart.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1LdNH96Q_ejNL6oDYMtqLF3qeAXKiWuuh
"""

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

data=pd.read_csv('heart.csv')

data.head(5)

data.info()

data.describe().T.style.background_gradient(cmap = "Blues")

count_series = data.groupby(['Sex', 'HeartDisease']).size()
data_Sex = count_series.to_frame(name = 'size').reset_index()
sns.barplot(data=data_Sex, x="Sex", y="size", hue="HeartDisease")

fig,ax = plt.subplots(figsize = (5,5))
plt.pie(data['Sex'].value_counts()[:20],labels = list(data['Sex'].value_counts()[:20].index),autopct = '%1.2f%%',labeldistance= 1.1)

count_series = data.groupby(['ChestPainType', 'HeartDisease']).size()
data_CPT = count_series.to_frame(name = 'size').reset_index()
sns.barplot(data=data_CPT, x="ChestPainType", y="size", hue="HeartDisease")

fig,ax = plt.subplots(figsize = (5,5))
plt.pie(data['ChestPainType'].value_counts()[:20],labels = list(data['ChestPainType'].value_counts()[:20].index),autopct = '%1.2f%%',labeldistance= 1.1)

count_series = data.groupby(['RestingECG', 'HeartDisease']).size()
data_Res = count_series.to_frame(name = 'size').reset_index()
sns.barplot(data=data_Res, x="RestingECG", y="size", hue="HeartDisease")

fig,ax = plt.subplots(figsize = (5,5))
plt.pie(data['RestingECG'].value_counts()[:20],labels = list(data['RestingECG'].value_counts()[:20].index),autopct = '%1.2f%%',labeldistance= 1.1)

count_series = data.groupby(['ST_Slope', 'HeartDisease']).size()
data_slope = count_series.to_frame(name = 'size').reset_index()
sns.barplot(data=data_slope, x="ST_Slope", y="size", hue="HeartDisease")

fig,ax = plt.subplots(figsize = (5,5))
plt.pie(data['ST_Slope'].value_counts()[:20],labels = list(data['ST_Slope'].value_counts()[:20].index),autopct = '%1.2f%%',labeldistance= 1.1)

plt.figure(figsize=(15,10))
for i,col in enumerate(data.columns,1):
    plt.subplot(4,3,i)
    plt.title(f"Distribution of {col} Data")
    sns.histplot(data[col],kde=True)
    plt.tight_layout()
    plt.plot()

plt.figure(figsize=(10,10))
sns.heatmap(data.corr(),annot=True,cmap='plasma')

plt.figure(figsize=(15,10))
sns.pairplot(data,hue="HeartDisease")
plt.legend("HeartDisease")
plt.tight_layout()
plt.plot()

new_data=pd.get_dummies(data,columns=['Sex','ChestPainType','RestingECG','ExerciseAngina','ST_Slope'],drop_first=True)

new_data

from sklearn.neighbors import LocalOutlierFactor
clf = LocalOutlierFactor()
y_pred = clf.fit_predict(new_data)

x_score = clf.negative_outlier_factor_
outlier_score = pd.DataFrame()
outlier_score["score"] = x_score

#threshold
threshold2 = -1.5
filtre2 = outlier_score["score"] < threshold2
outlier_index = outlier_score[filtre2].index.tolist()
len(outlier_index)

new_data.drop(outlier_index, inplace=True)

X=new_data.drop('HeartDisease',axis=1)
Y=new_data.HeartDisease

from imblearn import under_sampling 
from imblearn import over_sampling
from imblearn.over_sampling import SMOTE
from collections import Counter

sns.countplot(new_data["HeartDisease"], palette="Set3")
plt.title("HeartDisease ",fontsize=10)
plt.show()

sm = SMOTE(random_state=42)
print('Original dataset shape %s' % Counter(Y))
X, Y= sm.fit_resample(X, Y)
print('Resampled dataset shape %s' % Counter(Y))

sns.countplot(Y, palette='Set3')
plt.title("HeartDisease ",fontsize=10)
plt.show()

from sklearn.model_selection import train_test_split
X_train,X_test,Y_train,Y_test=train_test_split(X,Y,test_size=0.3,random_state=0)

from sklearn.preprocessing import StandardScaler
from sklearn.metrics import r2_score
scaler=StandardScaler()
X_train=scaler.fit_transform(X_train)
X_test=scaler.fit_transform(X_test)

from sklearn.metrics import r2_score 
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import GaussianNB
from sklearn.naive_bayes import BernoulliNB
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import SGDClassifier
from sklearn.pipeline import Pipeline
from sklearn.svm import NuSVC
from sklearn.ensemble import AdaBoostClassifier,GradientBoostingClassifier

pipeline_1=Pipeline([('scalar1',StandardScaler()),('lr',LogisticRegression())])
pipeline_2=Pipeline([('scalar2',StandardScaler()),('gnb',GaussianNB())])
pipeline_3=Pipeline([('scalar3',StandardScaler()),('bnb',BernoulliNB())])
pipeline_4=Pipeline([('scalar4',StandardScaler()),('knc',KNeighborsClassifier())])
pipeline_5=Pipeline([('scalar5',StandardScaler()),('dtc',DecisionTreeClassifier(random_state=0))])
pipeline_6=Pipeline([('scalar6',StandardScaler()),('svc',SVC(random_state=0))])
pipeline_7=Pipeline([('scalar7',StandardScaler()),('rfc',RandomForestClassifier(random_state=0))])
pipeline_8=Pipeline([('scalar8',StandardScaler()),('sgd',SGDClassifier(random_state=0))])
pipeline_9=Pipeline([('scalar9',StandardScaler()),('nusvc',NuSVC())])
pipeline_10=Pipeline([('scalar9',StandardScaler()),('ada',AdaBoostClassifier())])
pipeline_11=Pipeline([('scalar10',StandardScaler()),('gbc',GradientBoostingClassifier())])

list_pipelines=[pipeline_1,pipeline_2,pipeline_3,pipeline_4,pipeline_5,pipeline_6,pipeline_7,pipeline_8,pipeline_9,pipeline_10,pipeline_11]
best_accuracy=0.0
best_classifier=0
best_pipeline=""
pipeline_dictionary={0:'LogisticRegression',1:'GaussianNB',2:'BernoulliNB',3:'KNeighborsClassifier',4:'DecisionTreeClassifier',5:'SVC',6:'RandomForestClassifier',7:'SGDClassifier',8:'NU SVC',9:'AdaBoostClassifier',10:'GradientBoostingClassifier'}

for i in list_pipelines:
  i.fit(X_train,Y_train)

for i,model in enumerate(list_pipelines):
  print(" {} Test Accuracy: {}".format(pipeline_dictionary[i],np.mean(model.score(X_test,Y_test))))

for i,model in enumerate(list_pipelines):
  if model.score(X_test,Y_test)>best_accuracy:
    best_accuracy=model.score(X_test,Y_test)
    best_pipeline=model
    best_classifier=i

print("Classifier with best accuracy: {}".format(pipeline_dictionary[best_classifier]))

classes = ['Normal', 'Heart_Disease']
from yellowbrick.classifier import ROCAUC

from sklearn.metrics import classification_report
rfc=RandomForestClassifier()
rfc.fit(X_train,Y_train)
y_pred=rfc.predict(X_test)
print(classification_report(Y_test, y_pred))

from sklearn.model_selection import RandomizedSearchCV
param_random = {'max_depth': [50, 80, 100],'max_features': [2, 3, 4],'min_samples_leaf': [3, 4, 5],'min_samples_split': [8, 10, 12],'n_estimators': [100, 300, 500, 750, 1000]}
rfc=RandomForestClassifier(random_state=0)
random_search = RandomizedSearchCV(rfc, param_random, scoring='accuracy',cv=5, n_jobs=-1, verbose=0)
random_fit = random_search.fit(X_train, Y_train)

print('The best parameters for RandomForestClassifier: ',random_fit.best_params_)
print('The best score for RandomForestClassifier',random_fit.best_score_)

visualizer = ROCAUC(rfc, classes=classes)
visualizer.fit(X_train, Y_train)       
visualizer.score(X_test, Y_test)        
visualizer.show()

gbc=GradientBoostingClassifier()
gbc.fit(X_train,Y_train)
y_pred=gbc.predict(X_test)
print(classification_report(Y_test, y_pred))

param_random = {'loss':['deviance', 'exponential'],'learning_rate':np.linspace(0,1,10),'criterion':['friedman_mse', 'squared_error', 'mse', 'mae'],'max_depth': [50, 80, 100],'max_features': [2, 3, 4,'auto','sqrt','log2'],'min_samples_leaf': [3, 4, 5],'min_samples_split': [8, 10, 12],'n_estimators': [100, 300, 500, 750, 1000]}
gbc=GradientBoostingClassifier(random_state=0)
random_search = RandomizedSearchCV(gbc, param_random, scoring='accuracy',cv=5, n_jobs=-1, verbose=0)
random_fit = random_search.fit(X_train, Y_train)

print('The best parameters for GradientBoostingClassifier: ',random_fit.best_params_)
print('The best score for GradientBoostingClassifier',random_fit.best_score_)

visualizer = ROCAUC(gbc, classes=classes)
visualizer.fit(X_train, Y_train)       
visualizer.score(X_test, Y_test)        
visualizer.show()

knc=KNeighborsClassifier()
knc.fit(X_train,Y_train)
y_pred=knc.predict(X_test)
print(classification_report(Y_test, y_pred))

param_random={'n_neighbors':np.arange(3,20),'weights':['uniform','distance'],'algorithm':['auto', 'ball_tree', 'kd_tree', 'brute'],'leaf_size':np.arange(1,50),'p':[1,2]}
knc=KNeighborsClassifier()
random_search = RandomizedSearchCV(knc, param_random, scoring='accuracy',cv=5, n_jobs=-1, verbose=0)
random_fit = random_search.fit(X_train, Y_train)

print('The best parameters for KNieghborsClassifier: ',random_fit.best_params_)
print('The best score for KNieghborsClassifier',random_fit.best_score_)

visualizer = ROCAUC(knc, classes=classes)
visualizer.fit(X_train, Y_train)       
visualizer.score(X_test, Y_test)        
visualizer.show()

svc=SVC()
svc.fit(X_train,Y_train)
y_pred=svc.predict(X_test)
print(classification_report(Y_test, y_pred))